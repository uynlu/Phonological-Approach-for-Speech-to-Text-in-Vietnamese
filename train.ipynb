{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!gdown 1Zv2rByeWFgZGvyxkZkf0Ff37QTCLoJGm\n!gdown 1yWFeAkA7cfoFBIab115qFJCbw456MJmS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:11:21.326867Z","iopub.execute_input":"2024-12-22T06:11:21.327101Z","iopub.status.idle":"2024-12-22T06:11:30.576590Z","shell.execute_reply.started":"2024-12-22T06:11:21.327078Z","shell.execute_reply":"2024-12-22T06:11:30.575604Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1Zv2rByeWFgZGvyxkZkf0Ff37QTCLoJGm\nFrom (redirected): https://drive.google.com/uc?id=1Zv2rByeWFgZGvyxkZkf0Ff37QTCLoJGm&confirm=t&uuid=1c4b8a8f-c7b2-494f-9e22-5962d0ce6f05\nTo: /kaggle/working/model.py\n100%|██████████████████████████████████████| 13.4k/13.4k [00:00<00:00, 40.0MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1yWFeAkA7cfoFBIab115qFJCbw456MJmS\nFrom (redirected): https://drive.google.com/uc?id=1yWFeAkA7cfoFBIab115qFJCbw456MJmS&confirm=t&uuid=eb9d7578-dc7b-4d2e-9cc7-6e35c25958d4\nTo: /kaggle/working/utils.py\n100%|██████████████████████████████████████| 8.36k/8.36k [00:00<00:00, 32.1MB/s]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchaudio torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:11:30.577538Z","iopub.execute_input":"2024-12-22T06:11:30.577849Z","iopub.status.idle":"2024-12-22T06:11:34.937546Z","shell.execute_reply.started":"2024-12-22T06:11:30.577828Z","shell.execute_reply":"2024-12-22T06:11:34.936717Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport gc\nimport argparse\nimport torchaudio\nimport torch\nimport torch.nn.functional as F\nimport zipfile\nimport json\n\nfrom torch import nn\nfrom torchmetrics.text.wer import WordErrorRate\nfrom torch.utils.data import Dataset, random_split\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom model import ConformerEncoder, LSTMDecoder\nfrom utils import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:11:34.938679Z","iopub.execute_input":"2024-12-22T06:11:34.938957Z","iopub.status.idle":"2024-12-22T06:11:40.721022Z","shell.execute_reply.started":"2024-12-22T06:11:34.938935Z","shell.execute_reply":"2024-12-22T06:11:40.720103Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.mkdir('common_voice')\nos.mkdir('ViVOS')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:11:40.721846Z","iopub.execute_input":"2024-12-22T06:11:40.722184Z","iopub.status.idle":"2024-12-22T06:11:40.725865Z","shell.execute_reply.started":"2024-12-22T06:11:40.722163Z","shell.execute_reply":"2024-12-22T06:11:40.725159Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!gdown 1enxwIwmqJ7d4uDMlQyaL3eSU_4CsI6PE -O common_voice/dev.json\n!gdown 1BdVj69KW4YKqykfxCYa3vrS5R-HVEOKi -O common_voice/test.json\n!gdown 15MnfOokem_HomGxxwn1i4xs8lrFQwR6i -O common_voice/train.json\n!gdown 1wUw8JcTcjGbKWdKKbtpnx10Acd9fn9BS -O common_voice/voices.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:11:40.726695Z","iopub.execute_input":"2024-12-22T06:11:40.726944Z","iopub.status.idle":"2024-12-22T06:11:59.446125Z","shell.execute_reply.started":"2024-12-22T06:11:40.726924Z","shell.execute_reply":"2024-12-22T06:11:59.445139Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1enxwIwmqJ7d4uDMlQyaL3eSU_4CsI6PE\nTo: /kaggle/working/common_voice/dev.json\n100%|████████████████████████████████████████| 204k/204k [00:00<00:00, 73.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1BdVj69KW4YKqykfxCYa3vrS5R-HVEOKi\nTo: /kaggle/working/common_voice/test.json\n100%|█████████████████████████████████████████| 397k/397k [00:00<00:00, 114MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=15MnfOokem_HomGxxwn1i4xs8lrFQwR6i\nTo: /kaggle/working/common_voice/train.json\n100%|███████████████████████████████████████| 1.26M/1.26M [00:00<00:00, 127MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1wUw8JcTcjGbKWdKKbtpnx10Acd9fn9BS\nFrom (redirected): https://drive.google.com/uc?id=1wUw8JcTcjGbKWdKKbtpnx10Acd9fn9BS&confirm=t&uuid=0e275f6b-5b6b-4a43-9786-5e18f2397417\nTo: /kaggle/working/common_voice/voices.zip\n100%|████████████████████████████████████████| 405M/405M [00:06<00:00, 66.3MB/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!gdown 1oV2v0RBHX_Rqvra0YUrgoBgND0QC64Pc -O ViVOS/train.json\n!gdown 1obDaRybTfcOaGrl6mhAb5bMpMESs2lr5 -O ViVOS/test.json\n!gdown 1JoUgZ6uGPb5_iZTDinjF5pRzUvhk-4-n -O ViVOS/voices.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:11:59.447936Z","iopub.execute_input":"2024-12-22T06:11:59.448153Z","iopub.status.idle":"2024-12-22T06:12:19.714795Z","shell.execute_reply.started":"2024-12-22T06:11:59.448135Z","shell.execute_reply":"2024-12-22T06:12:19.713983Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1oV2v0RBHX_Rqvra0YUrgoBgND0QC64Pc\nTo: /kaggle/working/ViVOS/train.json\n100%|███████████████████████████████████████| 2.28M/2.28M [00:00<00:00, 160MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1obDaRybTfcOaGrl6mhAb5bMpMESs2lr5\nTo: /kaggle/working/ViVOS/test.json\n100%|████████████████████████████████████████| 135k/135k [00:00<00:00, 96.1MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1JoUgZ6uGPb5_iZTDinjF5pRzUvhk-4-n\nFrom (redirected): https://drive.google.com/uc?id=1JoUgZ6uGPb5_iZTDinjF5pRzUvhk-4-n&confirm=t&uuid=bc8baafc-f18e-4e0d-88f6-f3e54de4a5c4\nTo: /kaggle/working/ViVOS/voices.zip\n100%|███████████████████████████████████████| 1.48G/1.48G [00:11<00:00, 130MB/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"parser = argparse.ArgumentParser(\"conformer\")\nparser.add_argument('--data_dir', type=str, default='./data', help='location to download data')\nparser.add_argument('--checkpoint_path', type=str, default='model_best.pt', help='path to store/load checkpoints')\nparser.add_argument('--load_checkpoint', action='store_true', default=False, help='resume training from checkpoint')\nparser.add_argument('--train_set', type=str, default='train-clean-100', help='train dataset')\nparser.add_argument('--test_set', type=str, default='test-clean', help='test dataset')\nparser.add_argument('--batch_size', type=int, default=4, help='batch size')\nparser.add_argument('--warmup_steps', type=float, default=5000, help='Multiply by sqrt(d_model) to get max_lr')\nparser.add_argument('--peak_lr_ratio', type=int, default=0.01, help='Number of warmup steps for LR scheduler')\nparser.add_argument('--gpu', type=int, default=0, help='gpu device id (optional)')\nparser.add_argument('--epochs', type=int, default=50, help='num of training epochs')\nparser.add_argument('--report_freq', type=int, default=100, help='training objective report frequency')\nparser.add_argument('--layers', type=int, default=8, help='total number of layers')\nparser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\nparser.add_argument('--use_amp', action='store_true', default=False, help='use mixed precision to train')\nparser.add_argument('--attention_heads', type=int, default=4, help='number of heads to use for multi-head attention')\nparser.add_argument('--d_input', type=int, default=80, help='dimension of the input (num filter banks)')\nparser.add_argument('--d_encoder', type=int, default=128, help='dimension of the encoder')\nparser.add_argument('--d_decoder', type=int, default=256, help='dimension of the decoder')\nparser.add_argument('--encoder_layers', type=int, default=16, help='number of conformer blocks in the encoder')\nparser.add_argument('--decoder_layers', type=int, default=1, help='number of decoder layers')\nparser.add_argument('--conv_kernel_size', type=int, default=31, help='size of kernel for conformer convolution blocks')\nparser.add_argument('--feed_forward_expansion_factor', type=int, default=2, help='expansion factor for conformer feed forward blocks')\nparser.add_argument('--feed_forward_residual_factor', type=int, default=.3, help='residual factor for conformer feed forward blocks')\nparser.add_argument('--dropout', type=float, default=0.05, help='dropout factor for conformer model')\nparser.add_argument('--weight_decay', type=float, default=1e-5, help='model weight decay (corresponds to L2 regularization)')\nparser.add_argument('--variational_noise_std', type=float, default=1e-5, help='std of noise added to model weights for regularization')\nparser.add_argument('--num_workers', type=int, default=2, help='num_workers for the dataloader')\nparser.add_argument('--smart_batch', type=bool, default=True, help='Use smart batching for faster training')\nparser.add_argument('--accumulate_iters', type=int, default=1, help='Number of iterations to accumulate gradients')\nargs, unknown = parser.parse_known_args()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:15:14.209327Z","iopub.execute_input":"2024-12-22T06:15:14.209640Z","iopub.status.idle":"2024-12-22T06:15:14.221218Z","shell.execute_reply.started":"2024-12-22T06:15:14.209616Z","shell.execute_reply":"2024-12-22T06:15:14.220333Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def main():\n\n    #Load data\n    def read_json(file_path):\n        \"\"\"Đọc tệp JSON và trả về dữ liệu.\"\"\"\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        return data\n\n    def unzip_voices(zip_path, extract_to):\n        \"\"\"Giải nén tệp âm thanh từ file zip vào thư mục chỉ định.\"\"\"\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_to)\n            print(f\"Đã giải nén tệp vào: {extract_to}\")\n\n        # Xử lý trường hợp thư mục lồng nhau\n        nested_path = os.path.join(extract_to, 'voices')\n        if os.path.isdir(nested_path):\n            for item in os.listdir(nested_path):\n                nested_item_path = os.path.join(nested_path, item)\n                final_destination = os.path.join(extract_to, item)\n                os.rename(nested_item_path, final_destination)\n            os.rmdir(nested_path)\n            print(f\"Đã xử lý thư mục lồng nhau tại: {extract_to}\")\n\n    def create_voice_id_map(dataset):\n        \"\"\"Tạo bản đồ ánh xạ ID từ tên file âm thanh (hỗ trợ cả .wav và .mp3).\"\"\"\n        return {\n            value['voice'].replace('.wav', '').replace('.mp3', ''): value\n            for key, value in dataset.items() if isinstance(value, dict) and 'voice' in value\n        }\n\n    common_voice_train = read_json('/kaggle/working/common_voice/train.json')\n    common_voice_dev = read_json('/kaggle/working/common_voice/dev.json')\n    common_voice_test = read_json('/kaggle/working/common_voice/test.json')\n    vivos_train = read_json('/kaggle/working/ViVOS/train.json')\n    vivos_test = read_json('/kaggle/working/ViVOS/test.json')\n\n    common_voice_dataset = {**common_voice_train, **common_voice_dev, **common_voice_test}\n    vivos_dataset = {**vivos_train, **vivos_test}\n\n    print(f\"Số lượng mẫu common_voice: {len(common_voice_dataset)}\")\n    print(f\"Số lượng mẫu vivos: {len(vivos_dataset)}\")\n\n    common_voice_map = create_voice_id_map(common_voice_dataset)\n    vivos_map = create_voice_id_map(vivos_dataset)\n\n    class VoiceDataset(Dataset):\n        def __init__(self, merged_data):\n            \"\"\"Khởi tạo với dữ liệu đã ghép.\"\"\"\n            self.merged_data = merged_data\n\n        def __len__(self):\n            return len(self.merged_data)\n\n        def __getitem__(self, idx):\n            \"\"\"Trả về cặp âm thanh và văn bản.\"\"\"\n            audio_path, transcription = self.merged_data[idx]\n            audio, sample_rate = torchaudio.load(audio_path)\n            return audio, '', transcription, '', '', ''\n\n    common_voice_zip_path = '/kaggle/working/common_voice/voices.zip'\n    vivos_zip_path = '/kaggle/working/ViVOS/voices.zip'\n\n    common_voice_data_dir = '/kaggle/working/common_voice/voices'\n    vivos_data_dir = '/kaggle/working/ViVOS/voices'\n\n    unzip_voices(common_voice_zip_path, common_voice_data_dir)\n    unzip_voices(vivos_zip_path, vivos_data_dir)\n\n    def merge_audio_with_text(audio_dir, text_map):\n        \"\"\"Ghép dữ liệu âm thanh và văn bản dựa trên ID và chỉ lấy những phần tử đã ghép thành công.\"\"\"\n        merged_data = []\n\n        audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav') or f.endswith('.mp3')]\n\n        for audio_file in audio_files:\n            audio_id = audio_file.replace('.wav', '').replace('.mp3', '')\n\n            if audio_id in text_map:\n                text_data = text_map[audio_id]\n                transcription = text_data['script']\n\n                if transcription:\n                    audio_path = os.path.join(audio_dir, audio_file)\n                    merged_data.append((audio_path, transcription))\n\n        return merged_data\n\n    common_voice_merged = merge_audio_with_text(common_voice_data_dir, common_voice_map)\n    vivos_merged = merge_audio_with_text(vivos_data_dir, vivos_map)\n\n    full_dataset = common_voice_merged + vivos_merged\n    print(f\"Kích thước tập dữ liệu tổng hợp: {len(full_dataset)}\")\n\n    voice_dataset = VoiceDataset(full_dataset)\n\n    train_size = int(0.8 * len(voice_dataset))\n    test_size = len(voice_dataset) - train_size\n    train_data, test_data = random_split(voice_dataset, [train_size, test_size])\n\n    print(f\"Train data size: {len(train_data)}\")\n    print(f\"Test data size: {len(test_data)}\")\n\n    gpu = False\n\n    try:\n        print(\"Starting data loader setup...\")\n\n        # if args.smart_batch:\n        #     print('Sorting training data for smart batching...')\n        #     sorted_train_inds = [ind for ind, _ in sorted(enumerate(train_data), key=lambda x: x[1][0].shape[1])]\n        #     sorted_test_inds = [ind for ind, _ in sorted(enumerate(test_data), key=lambda x: x[1][0].shape[1])]\n        #     train_loader = DataLoader(dataset=train_data,\n        #                                     pin_memory=True,\n        #                                     num_workers=args.num_workers,\n        #                                     batch_sampler=BatchSampler(sorted_train_inds, batch_size=args.batch_size),\n        #                                     collate_fn=lambda x: preprocess_example(x, 'train'))\n\n        #     test_loader = DataLoader(dataset=test_data,\n        #                                 pin_memory=True,\n        #                                 num_workers=args.num_workers,\n        #                                 batch_sampler=BatchSampler(sorted_test_inds, batch_size=args.batch_size),\n        #                                 collate_fn=lambda x: preprocess_example(x, 'valid'))\n\n        # else:\n        print(\"Using regular batching...\")\n        train_loader = DataLoader(dataset=train_data,\n                                  pin_memory=True,\n                                  num_workers=0,\n                                  batch_size=args.batch_size,\n                                  shuffle=True,\n                                  collate_fn=lambda x: preprocess_example(x, 'train'))\n\n        test_loader = DataLoader(dataset=test_data,\n                                pin_memory=True,\n                                num_workers=0,\n                                batch_size=args.batch_size,\n                                shuffle=False,\n                                collate_fn=lambda x: preprocess_example(x, 'valid'))\n\n\n        # Declare Models\n        print(\"Initializing models...\")\n        encoder = ConformerEncoder(\n            d_input=args.d_input,\n            d_model=args.d_encoder,\n            num_layers=args.encoder_layers,\n            conv_kernel_size=args.conv_kernel_size,\n            dropout=args.dropout,\n            feed_forward_residual_factor=args.feed_forward_residual_factor,\n            feed_forward_expansion_factor=args.feed_forward_expansion_factor,\n            num_heads=args.attention_heads)\n\n        decoder = LSTMDecoder(\n            d_encoder=args.d_encoder,\n            d_decoder=args.d_decoder,\n            num_layers=args.decoder_layers)\n\n        char_decoder = GreedyCharacterDecoder().eval()\n\n        criterion = nn.CTCLoss(blank=8, zero_infinity=True)\n\n        # Optimizer and scheduler\n        optimizer = torch.optim.AdamW(\n            list(encoder.parameters()) + list(decoder.parameters()),\n            lr=0.01,\n            betas=(.9, .98),\n            eps=1e-05 if args.use_amp else 1e-09,\n            weight_decay=args.weight_decay\n            )\n\n        scheduler = TransformerLrScheduler(optimizer, args.d_encoder, args.warmup_steps)\n\n        # Print model size\n        model_size(encoder, 'Encoder')\n        model_size(decoder, 'Decoder')\n\n        # GPU Setup\n        print(\"Setting up GPU environment...\")\n        if torch.cuda.is_available():\n            print('Using GPU')\n            gpu = True\n            torch.cuda.set_device(args.gpu)\n            criterion = criterion.cuda()\n            encoder = encoder.cuda()\n            decoder = decoder.cuda()\n            char_decoder = char_decoder.cuda()\n            torch.cuda.empty_cache()\n        else:\n            print(\"GPU not available, using CPU\")\n            gpu = False\n\n        # Mixed Precision Setup\n        if args.use_amp:\n            print('Using Mixed Precision')\n        grad_scaler = torch.amp.GradScaler(enabled=args.use_amp)\n\n        # Initialize Checkpoint\n        print(\"Initializing checkpoint...\")\n        if args.load_checkpoint:\n            start_epoch, best_loss = load_checkpoint(encoder, decoder, optimizer, scheduler, args.checkpoint_path)\n            print(f'Resuming training from checkpoint starting at epoch {start_epoch}.')\n        else:\n            start_epoch = 0\n            best_loss = float('inf')\n\n        # Train Loop\n        print(\"Starting training loop...\")\n        optimizer.zero_grad()\n        for epoch in range(start_epoch, args.epochs):\n            torch.cuda.empty_cache()\n            print(f\"Starting epoch {epoch + 1} / {args.epochs}\")\n\n            # Variational noise for regularization\n            add_model_noise(encoder, std=args.variational_noise_std, gpu=gpu)\n            add_model_noise(decoder, std=args.variational_noise_std, gpu=gpu)\n\n            # Train/Validation loops\n            print(\"Training step...\")\n            wer, loss = train(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, args, gpu=gpu)\n            print(\"Validation step...\")\n            valid_wer, valid_loss = validate(encoder, decoder, char_decoder, criterion, test_loader, args, gpu=gpu)\n\n            print(f'Epoch {epoch} - Valid WER: {valid_wer}%, Valid Loss: {valid_loss}, Train WER: {wer}%, Train Loss: {loss}')\n\n            # Save checkpoint\n            if valid_loss <= best_loss:\n                print('Validation loss improved, saving checkpoint.')\n                best_loss = valid_loss\n                save_checkpoint(encoder, decoder, optimizer, scheduler, valid_loss, epoch+1, args.checkpoint_path)\n\n        print(\"Training loop completed successfully.\")\n\n    except Exception as e:\n        import traceback\n        print(\"An error occurred:\")\n        traceback.print_exc()\n        print(\"Error details:\", str(e))\n\n        # If error occurs, set up GPU again\n        print(\"Setting up GPU environment...\")\n        if torch.cuda.is_available():\n            print('Using GPU')\n            gpu = True\n            torch.cuda.set_device(args.gpu)\n            criterion = criterion.cuda()\n            encoder = encoder.cuda()\n            decoder = decoder.cuda()\n            char_decoder = char_decoder.cuda()\n            torch.cuda.empty_cache()\n        else:\n            print(\"GPU not available, using CPU\")\n            gpu = False\n\n        # Mixed Precision Setup\n        if args.use_amp:\n            print('Using Mixed Precision')\n        grad_scaler = torch.amp.GradScaler(enabled=args.use_amp)\n\n        # Initialize Checkpoint\n        print(\"Initializing checkpoint...\")\n        if args.load_checkpoint:\n            start_epoch, best_loss = load_checkpoint(encoder, decoder, optimizer, scheduler, args.checkpoint_path)\n            print(f'Resuming training from checkpoint starting at epoch {start_epoch}.')\n        else:\n            start_epoch = 0\n            best_loss = float('inf')\n\n        # Training loop can be resumed after error handling\n        optimizer.zero_grad()\n        for epoch in range(start_epoch, args.epochs):\n            torch.cuda.empty_cache()\n            print(f\"Starting epoch {epoch + 1} / {args.epochs}\")\n\n            # Variational noise for regularization\n            add_model_noise(encoder, std=args.variational_noise_std, gpu=gpu)\n            add_model_noise(decoder, std=args.variational_noise_std, gpu=gpu)\n\n            # Train/Validation loops\n            print(\"Training step...\")\n            wer, loss = train(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, args, gpu=gpu)\n            print(\"Validation step...\")\n            valid_wer, valid_loss = validate(encoder, decoder, char_decoder, criterion, test_loader, args, gpu=gpu)\n\n            print(f'Epoch {epoch} - Valid WER: {valid_wer}%, Valid Loss: {valid_loss}, Train WER: {wer}%, Train Loss: {loss}')\n\n            # Save checkpoint\n            if valid_loss <= best_loss:\n                print('Validation loss improved, saving checkpoint.')\n                best_loss = valid_loss\n                save_checkpoint(encoder, decoder, optimizer, scheduler, valid_loss, epoch+1, args.checkpoint_path)\n\n        print(\"Training loop completed successfully.\")\n\ndef train(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, args, gpu=True):\n  ''' Run a single training epoch '''\n\n  wer = WordErrorRate()\n  error_rate = AvgMeter()\n  avg_loss = AvgMeter()\n  text_transform = TextTransform()\n\n  encoder.train()\n  decoder.train()\n  for i, batch in enumerate(train_loader):\n    scheduler.step()\n    gc.collect()\n    spectrograms, labels, input_lengths, label_lengths, references, mask = batch\n\n    # Move to GPU\n    if gpu:\n      spectrograms = spectrograms.cuda()\n      labels = labels.cuda()\n      input_lengths = torch.tensor(input_lengths).cuda()\n      label_lengths = torch.tensor(label_lengths).cuda()\n      mask = mask.cuda()\n\n    # Update models\n    with torch.amp.autocast('cuda', enabled=args.use_amp):\n\n      outputs = encoder(spectrograms, mask)\n\n      outputs = decoder(outputs)\n      if torch.any(torch.isnan(outputs)):\n        print(\"NaN detected in outputs before softmax\")\n\n      loss = criterion(F.log_softmax(outputs, dim=-1).transpose(0, 1), labels, input_lengths, label_lengths)\n      print(f\"Step {i+1} - Loss: {loss.item()}\")\n      if torch.isnan(loss).any():\n        print(f\"NaN detected in loss at step {i+1}\")\n\n\n    grad_scaler.scale(loss).backward()\n\n    torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), max_norm=1.0)\n    if (i+1) % args.accumulate_iters == 0:\n      grad_scaler.step(optimizer)\n      grad_scaler.update()\n      optimizer.zero_grad()\n    avg_loss.update(loss.detach().item())\n\n    # Predict words, compute WER\n    inds = char_decoder(outputs.detach())\n    predictions = []\n    for sample in inds:\n      predictions.append(text_transform.int_to_text(sample))\n    error_rate.update(wer(predictions, references) * 100)\n\n    # Print metrics and predictions\n    if (i+1) % args.report_freq == 0:\n      print(f'Step {i+1} - Avg WER: {error_rate.avg}%, Avg Loss: {avg_loss.avg}')\n      print('Sample Predictions: ', predictions)\n    del spectrograms, labels, input_lengths, label_lengths, references, outputs, inds, predictions\n  return error_rate.avg, avg_loss.avg\n\ndef validate(encoder, decoder, char_decoder, criterion, test_loader, args, gpu=True):\n  ''' Evaluate model on test dataset. '''\n\n  avg_loss = AvgMeter()\n  error_rate = AvgMeter()\n  wer = WordErrorRate()\n  text_transform = TextTransform()\n\n  encoder.eval()\n  decoder.eval()\n  for i, batch in enumerate(test_loader):\n    gc.collect()\n    spectrograms, labels, input_lengths, label_lengths, references, mask = batch\n\n    # Move to GPU\n    if gpu:\n      spectrograms = spectrograms.cuda()\n      labels = labels.cuda()\n      input_lengths = torch.tensor(input_lengths).cuda()\n      label_lengths = torch.tensor(label_lengths).cuda()\n      mask = mask.cuda()\n\n    with torch.no_grad():\n      with torch.amp.autocast('cuda', enabled=args.use_amp):\n        outputs = encoder(spectrograms, mask)\n        outputs = decoder(outputs)\n        loss = criterion(F.log_softmax(outputs, dim=-1).transpose(0, 1), labels, input_lengths, label_lengths)\n      avg_loss.update(loss.item())\n\n      inds = char_decoder(outputs.detach())\n      predictions = []\n      for sample in inds:\n        predictions.append(text_transform.int_to_text(sample))\n      error_rate.update(wer(predictions, references) * 100)\n  return error_rate.avg, avg_loss.avg\n\n\nif __name__ == '__main__':\n  main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:14:31.698006Z","iopub.execute_input":"2024-12-22T06:14:31.698327Z","iopub.status.idle":"2024-12-22T06:14:57.318034Z","shell.execute_reply.started":"2024-12-22T06:14:31.698299Z","shell.execute_reply":"2024-12-22T06:14:57.316912Z"}},"outputs":[{"name":"stdout","text":"Số lượng mẫu common_voice: 5717\nSố lượng mẫu vivos: 12420\nĐã giải nén tệp vào: /kaggle/working/common_voice/voices\nĐã xử lý thư mục lồng nhau tại: /kaggle/working/common_voice/voices\nĐã giải nén tệp vào: /kaggle/working/ViVOS/voices\nĐã xử lý thư mục lồng nhau tại: /kaggle/working/ViVOS/voices\nKích thước tập dữ liệu tổng hợp: 18137\nTrain data size: 14509\nTest data size: 3628\nStarting data loader setup...\nUsing regular batching...\nInitializing models...\nEncoder - num_params: 6.06M,  size: 23.12MB\nDecoder - num_params: 0.4M,  size: 1.54MB\nSetting up GPU environment...\nUsing GPU\nInitializing checkpoint...\nStarting training loop...\nStarting epoch 1 / 50\nTraining step...\nStep 1 - Loss: 10.681987762451172\nNaN detected in outputs before softmax\nStep 2 - Loss: nan\nNaN detected in loss at step 2\nNaN detected in outputs before softmax\nStep 3 - Loss: nan\nNaN detected in loss at step 3\nNaN detected in outputs before softmax\nStep 4 - Loss: nan\nNaN detected in loss at step 4\nNaN detected in outputs before softmax\nStep 5 - Loss: nan\nNaN detected in loss at step 5\nNaN detected in outputs before softmax\nStep 6 - Loss: nan\nNaN detected in loss at step 6\nNaN detected in outputs before softmax\nStep 7 - Loss: nan\nNaN detected in loss at step 7\nNaN detected in outputs before softmax\nStep 8 - Loss: nan\nNaN detected in loss at step 8\nNaN detected in outputs before softmax\nStep 9 - Loss: nan\nNaN detected in loss at step 9\nNaN detected in outputs before softmax\nStep 10 - Loss: nan\nNaN detected in loss at step 10\nNaN detected in outputs before softmax\nStep 11 - Loss: nan\nNaN detected in loss at step 11\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-b8c30b59c810>\u001b[0m in \u001b[0;36m<cell line: 394>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-b8c30b59c810>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;31m# Train/Validation loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training step...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mwer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation step...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mvalid_wer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-b8c30b59c810>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, args, gpu)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10}]}