{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T06:11:21.327101Z",
     "iopub.status.busy": "2024-12-22T06:11:21.326867Z",
     "iopub.status.idle": "2024-12-22T06:11:30.576590Z",
     "shell.execute_reply": "2024-12-22T06:11:30.575604Z",
     "shell.execute_reply.started": "2024-12-22T06:11:21.327078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!gdown 1Zv2rByeWFgZGvyxkZkf0Ff37QTCLoJGm\n",
    "!gdown 1yWFeAkA7cfoFBIab115qFJCbw456MJmS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T06:11:30.577849Z",
     "iopub.status.busy": "2024-12-22T06:11:30.577538Z",
     "iopub.status.idle": "2024-12-22T06:11:34.937546Z",
     "shell.execute_reply": "2024-12-22T06:11:34.936717Z",
     "shell.execute_reply.started": "2024-12-22T06:11:30.577828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchaudio torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T06:11:34.938957Z",
     "iopub.status.busy": "2024-12-22T06:11:34.938679Z",
     "iopub.status.idle": "2024-12-22T06:11:40.721022Z",
     "shell.execute_reply": "2024-12-22T06:11:40.720103Z",
     "shell.execute_reply.started": "2024-12-22T06:11:34.938935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "from torch import nn\n",
    "from torchmetrics.text.wer import WordErrorRate\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from model import ConformerEncoder, LSTMDecoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T06:11:40.722184Z",
     "iopub.status.busy": "2024-12-22T06:11:40.721846Z",
     "iopub.status.idle": "2024-12-22T06:11:40.725865Z",
     "shell.execute_reply": "2024-12-22T06:11:40.725159Z",
     "shell.execute_reply.started": "2024-12-22T06:11:40.722163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('common_voice')\n",
    "os.mkdir('ViVOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T06:11:40.726944Z",
     "iopub.status.busy": "2024-12-22T06:11:40.726695Z",
     "iopub.status.idle": "2024-12-22T06:11:59.446125Z",
     "shell.execute_reply": "2024-12-22T06:11:59.445139Z",
     "shell.execute_reply.started": "2024-12-22T06:11:40.726924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!gdown 1enxwIwmqJ7d4uDMlQyaL3eSU_4CsI6PE -O common_voice/dev.json\n",
    "!gdown 1BdVj69KW4YKqykfxCYa3vrS5R-HVEOKi -O common_voice/test.json\n",
    "!gdown 15MnfOokem_HomGxxwn1i4xs8lrFQwR6i -O common_voice/train.json\n",
    "!gdown 1wUw8JcTcjGbKWdKKbtpnx10Acd9fn9BS -O common_voice/voices.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T06:11:59.448153Z",
     "iopub.status.busy": "2024-12-22T06:11:59.447936Z",
     "iopub.status.idle": "2024-12-22T06:12:19.714795Z",
     "shell.execute_reply": "2024-12-22T06:12:19.713983Z",
     "shell.execute_reply.started": "2024-12-22T06:11:59.448135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!gdown 1oV2v0RBHX_Rqvra0YUrgoBgND0QC64Pc -O ViVOS/train.json\n",
    "!gdown 1obDaRybTfcOaGrl6mhAb5bMpMESs2lr5 -O ViVOS/test.json\n",
    "!gdown 1JoUgZ6uGPb5_iZTDinjF5pRzUvhk-4-n -O ViVOS/voices.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T06:15:14.209640Z",
     "iopub.status.busy": "2024-12-22T06:15:14.209327Z",
     "iopub.status.idle": "2024-12-22T06:15:14.221218Z",
     "shell.execute_reply": "2024-12-22T06:15:14.220333Z",
     "shell.execute_reply.started": "2024-12-22T06:15:14.209616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"conformer\")\n",
    "parser.add_argument('--data_dir', type=str, default='./data', help='location to download data')\n",
    "parser.add_argument('--checkpoint_path', type=str, default='model_best.pt', help='path to store/load checkpoints')\n",
    "parser.add_argument('--load_checkpoint', action='store_true', default=False, help='resume training from checkpoint')\n",
    "parser.add_argument('--train_set', type=str, default='train-clean-100', help='train dataset')\n",
    "parser.add_argument('--test_set', type=str, default='test-clean', help='test dataset')\n",
    "parser.add_argument('--batch_size', type=int, default=4, help='batch size')\n",
    "parser.add_argument('--warmup_steps', type=float, default=5000, help='Multiply by sqrt(d_model) to get max_lr')\n",
    "parser.add_argument('--peak_lr_ratio', type=int, default=0.01, help='Number of warmup steps for LR scheduler')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id (optional)')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='num of training epochs')\n",
    "parser.add_argument('--report_freq', type=int, default=100, help='training objective report frequency')\n",
    "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--use_amp', action='store_true', default=False, help='use mixed precision to train')\n",
    "parser.add_argument('--attention_heads', type=int, default=4, help='number of heads to use for multi-head attention')\n",
    "parser.add_argument('--d_input', type=int, default=80, help='dimension of the input (num filter banks)')\n",
    "parser.add_argument('--d_encoder', type=int, default=128, help='dimension of the encoder')\n",
    "parser.add_argument('--d_decoder', type=int, default=256, help='dimension of the decoder')\n",
    "parser.add_argument('--encoder_layers', type=int, default=16, help='number of conformer blocks in the encoder')\n",
    "parser.add_argument('--decoder_layers', type=int, default=1, help='number of decoder layers')\n",
    "parser.add_argument('--conv_kernel_size', type=int, default=31, help='size of kernel for conformer convolution blocks')\n",
    "parser.add_argument('--feed_forward_expansion_factor', type=int, default=2, help='expansion factor for conformer feed forward blocks')\n",
    "parser.add_argument('--feed_forward_residual_factor', type=int, default=.3, help='residual factor for conformer feed forward blocks')\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout factor for conformer model')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5, help='model weight decay (corresponds to L2 regularization)')\n",
    "parser.add_argument('--variational_noise_std', type=float, default=1e-5, help='std of noise added to model weights for regularization')\n",
    "parser.add_argument('--num_workers', type=int, default=2, help='num_workers for the dataloader')\n",
    "parser.add_argument('--smart_batch', type=bool, default=True, help='Use smart batching for faster training')\n",
    "parser.add_argument('--accumulate_iters', type=int, default=1, help='Number of iterations to accumulate gradients')\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T06:14:31.698327Z",
     "iopub.status.busy": "2024-12-22T06:14:31.698006Z",
     "iopub.status.idle": "2024-12-22T06:14:57.318034Z",
     "shell.execute_reply": "2024-12-22T06:14:57.316912Z",
     "shell.execute_reply.started": "2024-12-22T06:14:31.698299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    #Load data\n",
    "    def read_json(file_path):\n",
    "        \"\"\"Đọc tệp JSON và trả về dữ liệu.\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def unzip_voices(zip_path, extract_to):\n",
    "        \"\"\"Giải nén tệp âm thanh từ file zip vào thư mục chỉ định.\"\"\"\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "            print(f\"Đã giải nén tệp vào: {extract_to}\")\n",
    "\n",
    "        # Xử lý trường hợp thư mục lồng nhau\n",
    "        nested_path = os.path.join(extract_to, 'voices')\n",
    "        if os.path.isdir(nested_path):\n",
    "            for item in os.listdir(nested_path):\n",
    "                nested_item_path = os.path.join(nested_path, item)\n",
    "                final_destination = os.path.join(extract_to, item)\n",
    "                os.rename(nested_item_path, final_destination)\n",
    "            os.rmdir(nested_path)\n",
    "            print(f\"Đã xử lý thư mục lồng nhau tại: {extract_to}\")\n",
    "\n",
    "    def create_voice_id_map(dataset):\n",
    "        \"\"\"Tạo bản đồ ánh xạ ID từ tên file âm thanh (hỗ trợ cả .wav và .mp3).\"\"\"\n",
    "        return {\n",
    "            value['voice'].replace('.wav', '').replace('.mp3', ''): value\n",
    "            for key, value in dataset.items() if isinstance(value, dict) and 'voice' in value\n",
    "        }\n",
    "\n",
    "    common_voice_train = read_json('/kaggle/working/common_voice/train.json')\n",
    "    common_voice_dev = read_json('/kaggle/working/common_voice/dev.json')\n",
    "    common_voice_test = read_json('/kaggle/working/common_voice/test.json')\n",
    "    vivos_train = read_json('/kaggle/working/ViVOS/train.json')\n",
    "    vivos_test = read_json('/kaggle/working/ViVOS/test.json')\n",
    "\n",
    "    common_voice_dataset = {**common_voice_train, **common_voice_dev, **common_voice_test}\n",
    "    vivos_dataset = {**vivos_train, **vivos_test}\n",
    "\n",
    "    print(f\"Số lượng mẫu common_voice: {len(common_voice_dataset)}\")\n",
    "    print(f\"Số lượng mẫu vivos: {len(vivos_dataset)}\")\n",
    "\n",
    "    common_voice_map = create_voice_id_map(common_voice_dataset)\n",
    "    vivos_map = create_voice_id_map(vivos_dataset)\n",
    "\n",
    "    class VoiceDataset(Dataset):\n",
    "        def __init__(self, merged_data):\n",
    "            \"\"\"Khởi tạo với dữ liệu đã ghép.\"\"\"\n",
    "            self.merged_data = merged_data\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.merged_data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            \"\"\"Trả về cặp âm thanh và văn bản.\"\"\"\n",
    "            audio_path, transcription = self.merged_data[idx]\n",
    "            audio, sample_rate = torchaudio.load(audio_path)\n",
    "            return audio, '', transcription, '', '', ''\n",
    "\n",
    "    common_voice_zip_path = '/kaggle/working/common_voice/voices.zip'\n",
    "    vivos_zip_path = '/kaggle/working/ViVOS/voices.zip'\n",
    "\n",
    "    common_voice_data_dir = '/kaggle/working/common_voice/voices'\n",
    "    vivos_data_dir = '/kaggle/working/ViVOS/voices'\n",
    "\n",
    "    unzip_voices(common_voice_zip_path, common_voice_data_dir)\n",
    "    unzip_voices(vivos_zip_path, vivos_data_dir)\n",
    "\n",
    "    def merge_audio_with_text(audio_dir, text_map):\n",
    "        \"\"\"Ghép dữ liệu âm thanh và văn bản dựa trên ID và chỉ lấy những phần tử đã ghép thành công.\"\"\"\n",
    "        merged_data = []\n",
    "\n",
    "        audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav') or f.endswith('.mp3')]\n",
    "\n",
    "        for audio_file in audio_files:\n",
    "            audio_id = audio_file.replace('.wav', '').replace('.mp3', '')\n",
    "\n",
    "            if audio_id in text_map:\n",
    "                text_data = text_map[audio_id]\n",
    "                transcription = text_data['script']\n",
    "\n",
    "                if transcription:\n",
    "                    audio_path = os.path.join(audio_dir, audio_file)\n",
    "                    merged_data.append((audio_path, transcription))\n",
    "\n",
    "        return merged_data\n",
    "\n",
    "    common_voice_merged = merge_audio_with_text(common_voice_data_dir, common_voice_map)\n",
    "    vivos_merged = merge_audio_with_text(vivos_data_dir, vivos_map)\n",
    "\n",
    "    full_dataset = common_voice_merged + vivos_merged\n",
    "    print(f\"Kích thước tập dữ liệu tổng hợp: {len(full_dataset)}\")\n",
    "\n",
    "    voice_dataset = VoiceDataset(full_dataset)\n",
    "\n",
    "    train_size = int(0.8 * len(voice_dataset))\n",
    "    test_size = len(voice_dataset) - train_size\n",
    "    train_data, test_data = random_split(voice_dataset, [train_size, test_size])\n",
    "\n",
    "    print(f\"Train data size: {len(train_data)}\")\n",
    "    print(f\"Test data size: {len(test_data)}\")\n",
    "\n",
    "    gpu = False\n",
    "\n",
    "    try:\n",
    "        print(\"Starting data loader setup...\")\n",
    "\n",
    "        # if args.smart_batch:\n",
    "        #     print('Sorting training data for smart batching...')\n",
    "        #     sorted_train_inds = [ind for ind, _ in sorted(enumerate(train_data), key=lambda x: x[1][0].shape[1])]\n",
    "        #     sorted_test_inds = [ind for ind, _ in sorted(enumerate(test_data), key=lambda x: x[1][0].shape[1])]\n",
    "        #     train_loader = DataLoader(dataset=train_data,\n",
    "        #                                     pin_memory=True,\n",
    "        #                                     num_workers=args.num_workers,\n",
    "        #                                     batch_sampler=BatchSampler(sorted_train_inds, batch_size=args.batch_size),\n",
    "        #                                     collate_fn=lambda x: preprocess_example(x, 'train'))\n",
    "\n",
    "        #     test_loader = DataLoader(dataset=test_data,\n",
    "        #                                 pin_memory=True,\n",
    "        #                                 num_workers=args.num_workers,\n",
    "        #                                 batch_sampler=BatchSampler(sorted_test_inds, batch_size=args.batch_size),\n",
    "        #                                 collate_fn=lambda x: preprocess_example(x, 'valid'))\n",
    "\n",
    "        # else:\n",
    "        print(\"Using regular batching...\")\n",
    "        train_loader = DataLoader(dataset=train_data,\n",
    "                                  pin_memory=True,\n",
    "                                  num_workers=0,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  collate_fn=lambda x: preprocess_example(x, 'train'))\n",
    "\n",
    "        test_loader = DataLoader(dataset=test_data,\n",
    "                                pin_memory=True,\n",
    "                                num_workers=0,\n",
    "                                batch_size=args.batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: preprocess_example(x, 'valid'))\n",
    "\n",
    "\n",
    "        # Declare Models\n",
    "        print(\"Initializing models...\")\n",
    "        encoder = ConformerEncoder(\n",
    "            d_input=args.d_input,\n",
    "            d_model=args.d_encoder,\n",
    "            num_layers=args.encoder_layers,\n",
    "            conv_kernel_size=args.conv_kernel_size,\n",
    "            dropout=args.dropout,\n",
    "            feed_forward_residual_factor=args.feed_forward_residual_factor,\n",
    "            feed_forward_expansion_factor=args.feed_forward_expansion_factor,\n",
    "            num_heads=args.attention_heads)\n",
    "\n",
    "        decoder = LSTMDecoder(\n",
    "            d_encoder=args.d_encoder,\n",
    "            d_decoder=args.d_decoder,\n",
    "            num_layers=args.decoder_layers)\n",
    "\n",
    "        char_decoder = GreedyCharacterDecoder().eval()\n",
    "\n",
    "        criterion = nn.CTCLoss(blank=8, zero_infinity=True)\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            list(encoder.parameters()) + list(decoder.parameters()),\n",
    "            lr=0.01,\n",
    "            betas=(.9, .98),\n",
    "            eps=1e-05 if args.use_amp else 1e-09,\n",
    "            weight_decay=args.weight_decay\n",
    "            )\n",
    "\n",
    "        scheduler = TransformerLrScheduler(optimizer, args.d_encoder, args.warmup_steps)\n",
    "\n",
    "        # Print model size\n",
    "        model_size(encoder, 'Encoder')\n",
    "        model_size(decoder, 'Decoder')\n",
    "\n",
    "        # GPU Setup\n",
    "        print(\"Setting up GPU environment...\")\n",
    "        if torch.cuda.is_available():\n",
    "            print('Using GPU')\n",
    "            gpu = True\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            criterion = criterion.cuda()\n",
    "            encoder = encoder.cuda()\n",
    "            decoder = decoder.cuda()\n",
    "            char_decoder = char_decoder.cuda()\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print(\"GPU not available, using CPU\")\n",
    "            gpu = False\n",
    "\n",
    "        # Mixed Precision Setup\n",
    "        if args.use_amp:\n",
    "            print('Using Mixed Precision')\n",
    "        grad_scaler = torch.amp.GradScaler(enabled=args.use_amp)\n",
    "\n",
    "        # Initialize Checkpoint\n",
    "        print(\"Initializing checkpoint...\")\n",
    "        if args.load_checkpoint:\n",
    "            start_epoch, best_loss = load_checkpoint(encoder, decoder, optimizer, scheduler, args.checkpoint_path)\n",
    "            print(f'Resuming training from checkpoint starting at epoch {start_epoch}.')\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            best_loss = float('inf')\n",
    "\n",
    "        # Train Loop\n",
    "        print(\"Starting training loop...\")\n",
    "        optimizer.zero_grad()\n",
    "        for epoch in range(start_epoch, args.epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"Starting epoch {epoch + 1} / {args.epochs}\")\n",
    "\n",
    "            # Variational noise for regularization\n",
    "            add_model_noise(encoder, std=args.variational_noise_std, gpu=gpu)\n",
    "            add_model_noise(decoder, std=args.variational_noise_std, gpu=gpu)\n",
    "\n",
    "            # Train/Validation loops\n",
    "            print(\"Training step...\")\n",
    "            wer, loss = train(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, args, gpu=gpu)\n",
    "            print(\"Validation step...\")\n",
    "            valid_wer, valid_loss = validate(encoder, decoder, char_decoder, criterion, test_loader, args, gpu=gpu)\n",
    "\n",
    "            print(f'Epoch {epoch} - Valid WER: {valid_wer}%, Valid Loss: {valid_loss}, Train WER: {wer}%, Train Loss: {loss}')\n",
    "\n",
    "            # Save checkpoint\n",
    "            if valid_loss <= best_loss:\n",
    "                print('Validation loss improved, saving checkpoint.')\n",
    "                best_loss = valid_loss\n",
    "                save_checkpoint(encoder, decoder, optimizer, scheduler, valid_loss, epoch+1, args.checkpoint_path)\n",
    "\n",
    "        print(\"Training loop completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"An error occurred:\")\n",
    "        traceback.print_exc()\n",
    "        print(\"Error details:\", str(e))\n",
    "\n",
    "        # If error occurs, set up GPU again\n",
    "        print(\"Setting up GPU environment...\")\n",
    "        if torch.cuda.is_available():\n",
    "            print('Using GPU')\n",
    "            gpu = True\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            criterion = criterion.cuda()\n",
    "            encoder = encoder.cuda()\n",
    "            decoder = decoder.cuda()\n",
    "            char_decoder = char_decoder.cuda()\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print(\"GPU not available, using CPU\")\n",
    "            gpu = False\n",
    "\n",
    "        # Mixed Precision Setup\n",
    "        if args.use_amp:\n",
    "            print('Using Mixed Precision')\n",
    "        grad_scaler = torch.amp.GradScaler(enabled=args.use_amp)\n",
    "\n",
    "        # Initialize Checkpoint\n",
    "        print(\"Initializing checkpoint...\")\n",
    "        if args.load_checkpoint:\n",
    "            start_epoch, best_loss = load_checkpoint(encoder, decoder, optimizer, scheduler, args.checkpoint_path)\n",
    "            print(f'Resuming training from checkpoint starting at epoch {start_epoch}.')\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            best_loss = float('inf')\n",
    "\n",
    "        # Training loop can be resumed after error handling\n",
    "        optimizer.zero_grad()\n",
    "        for epoch in range(start_epoch, args.epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"Starting epoch {epoch + 1} / {args.epochs}\")\n",
    "\n",
    "            # Variational noise for regularization\n",
    "            add_model_noise(encoder, std=args.variational_noise_std, gpu=gpu)\n",
    "            add_model_noise(decoder, std=args.variational_noise_std, gpu=gpu)\n",
    "\n",
    "            # Train/Validation loops\n",
    "            print(\"Training step...\")\n",
    "            wer, loss = train(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, args, gpu=gpu)\n",
    "            print(\"Validation step...\")\n",
    "            valid_wer, valid_loss = validate(encoder, decoder, char_decoder, criterion, test_loader, args, gpu=gpu)\n",
    "\n",
    "            print(f'Epoch {epoch} - Valid WER: {valid_wer}%, Valid Loss: {valid_loss}, Train WER: {wer}%, Train Loss: {loss}')\n",
    "\n",
    "            # Save checkpoint\n",
    "            if valid_loss <= best_loss:\n",
    "                print('Validation loss improved, saving checkpoint.')\n",
    "                best_loss = valid_loss\n",
    "                save_checkpoint(encoder, decoder, optimizer, scheduler, valid_loss, epoch+1, args.checkpoint_path)\n",
    "\n",
    "        print(\"Training loop completed successfully.\")\n",
    "\n",
    "def train(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, args, gpu=True):\n",
    "  ''' Run a single training epoch '''\n",
    "\n",
    "  wer = WordErrorRate()\n",
    "  error_rate = AvgMeter()\n",
    "  avg_loss = AvgMeter()\n",
    "  text_transform = TextTransform()\n",
    "\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    scheduler.step()\n",
    "    gc.collect()\n",
    "    spectrograms, labels, input_lengths, label_lengths, references, mask = batch\n",
    "\n",
    "    # Move to GPU\n",
    "    if gpu:\n",
    "      spectrograms = spectrograms.cuda()\n",
    "      labels = labels.cuda()\n",
    "      input_lengths = torch.tensor(input_lengths).cuda()\n",
    "      label_lengths = torch.tensor(label_lengths).cuda()\n",
    "      mask = mask.cuda()\n",
    "\n",
    "    # Update models\n",
    "    with torch.amp.autocast('cuda', enabled=args.use_amp):\n",
    "\n",
    "      outputs = encoder(spectrograms, mask)\n",
    "\n",
    "      outputs = decoder(outputs)\n",
    "      if torch.any(torch.isnan(outputs)):\n",
    "        print(\"NaN detected in outputs before softmax\")\n",
    "\n",
    "      loss = criterion(F.log_softmax(outputs, dim=-1).transpose(0, 1), labels, input_lengths, label_lengths)\n",
    "      print(f\"Step {i+1} - Loss: {loss.item()}\")\n",
    "      if torch.isnan(loss).any():\n",
    "        print(f\"NaN detected in loss at step {i+1}\")\n",
    "\n",
    "\n",
    "    grad_scaler.scale(loss).backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), max_norm=1.0)\n",
    "    if (i+1) % args.accumulate_iters == 0:\n",
    "      grad_scaler.step(optimizer)\n",
    "      grad_scaler.update()\n",
    "      optimizer.zero_grad()\n",
    "    avg_loss.update(loss.detach().item())\n",
    "\n",
    "    # Predict words, compute WER\n",
    "    inds = char_decoder(outputs.detach())\n",
    "    predictions = []\n",
    "    for sample in inds:\n",
    "      predictions.append(text_transform.int_to_text(sample))\n",
    "    error_rate.update(wer(predictions, references) * 100)\n",
    "\n",
    "    # Print metrics and predictions\n",
    "    if (i+1) % args.report_freq == 0:\n",
    "      print(f'Step {i+1} - Avg WER: {error_rate.avg}%, Avg Loss: {avg_loss.avg}')\n",
    "      print('Sample Predictions: ', predictions)\n",
    "    del spectrograms, labels, input_lengths, label_lengths, references, outputs, inds, predictions\n",
    "  return error_rate.avg, avg_loss.avg\n",
    "\n",
    "def validate(encoder, decoder, char_decoder, criterion, test_loader, args, gpu=True):\n",
    "  ''' Evaluate model on test dataset. '''\n",
    "\n",
    "  avg_loss = AvgMeter()\n",
    "  error_rate = AvgMeter()\n",
    "  wer = WordErrorRate()\n",
    "  text_transform = TextTransform()\n",
    "\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  for i, batch in enumerate(test_loader):\n",
    "    gc.collect()\n",
    "    spectrograms, labels, input_lengths, label_lengths, references, mask = batch\n",
    "\n",
    "    # Move to GPU\n",
    "    if gpu:\n",
    "      spectrograms = spectrograms.cuda()\n",
    "      labels = labels.cuda()\n",
    "      input_lengths = torch.tensor(input_lengths).cuda()\n",
    "      label_lengths = torch.tensor(label_lengths).cuda()\n",
    "      mask = mask.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      with torch.amp.autocast('cuda', enabled=args.use_amp):\n",
    "        outputs = encoder(spectrograms, mask)\n",
    "        outputs = decoder(outputs)\n",
    "        loss = criterion(F.log_softmax(outputs, dim=-1).transpose(0, 1), labels, input_lengths, label_lengths)\n",
    "      avg_loss.update(loss.item())\n",
    "\n",
    "      inds = char_decoder(outputs.detach())\n",
    "      predictions = []\n",
    "      for sample in inds:\n",
    "        predictions.append(text_transform.int_to_text(sample))\n",
    "      error_rate.update(wer(predictions, references) * 100)\n",
    "  return error_rate.avg, avg_loss.avg\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
